{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is a large data set. The decompressed files require about 22GB of space.\n",
    "\n",
    "This data captures the process of offering incentives (a.k.a. coupons) to a large number of customers and forecasting those who will become loyal to the product. Let's say 100 customers are offered a discount to purchase two bottles of water. Of the 100 customers, 60 choose to redeem the offer. These 60 customers are the focus of this competition. You are asked to predict which of the 60 will return (during or after the promotional period) to purchase the same item again.\n",
    "\n",
    "To create this prediction, you are given a minimum of a year of shopping history prior to each customer's incentive, as well as the purchase histories of many other shoppers (some of whom will have received the same offer). The transaction history contains all items purchased, not just items related to the offer. Only one offer per customer is included in the data. The training set is comprised of offers issued before 2013-05-01. The test set is offers issued on or after 2013-05-01.\n",
    "\n",
    "Files\n",
    "You are provided four relational files:\n",
    "\n",
    "transactions.csv - contains transaction history for all customers for a period of at least 1 year prior to their offered incentive <br>\n",
    "trainHistory.csv - contains the incentive offered to each customer and information about the behavioral response to the offer <br>\n",
    "testHistory.csv - contains the incentive offered to each customer but does not include their response (you are predicting the repeater column for each id in this file) <br>\n",
    "offers.csv - contains information about the offers <br>\n",
    "\n",
    "Fields\n",
    "All of the fields are anonymized and categorized to protect customer and sales information. The specific meanings of the fields will not be provided (so don't bother asking). Part of the challenge of this competition is learning the taxonomy of items in a data-driven way.\n",
    "\n",
    "history\n",
    "id - A unique id representing a customer <br>\n",
    "chain - An integer representing a store chain <br>\n",
    "offer - An id representing a certain offer <br>\n",
    "market - An id representing a geographical region <br>\n",
    "repeattrips - The number of times the customer made a repeat purchase <br>\n",
    "repeater - A boolean, equal to repeattrips > 0 <br>\n",
    "offerdate - The date a customer received the offer <br>\n",
    "\n",
    "transactions\n",
    "id - see above <br>\n",
    "chain - see above <br>\n",
    "dept - An aggregate grouping of the Category (e.g. water) <br>\n",
    "category - The product category (e.g. sparkling water) <br>\n",
    "company - An id of the company that sells the item <br>\n",
    "brand - An id of the brand to which the item belongs <br>\n",
    "date - The date of purchase <br>\n",
    "productsize - The amount of the product purchase (e.g. 16 oz of water) <br>\n",
    "productmeasure - The units of the product purchase (e.g. ounces) <br>\n",
    "purchasequantity - The number of units purchased <br>\n",
    "purchaseamount - The dollar amount of the purchase <br>\n",
    "\n",
    "offers\n",
    "offer - see above <br>\n",
    "category - see above <br>\n",
    "quantity - The number of units one must purchase to get the discount <br>\n",
    "company - see above <br>\n",
    "offervalue - The dollar value of the offer <br>\n",
    "brand - see above <br>\n",
    "\n",
    "The transactions file can be joined to the history file by (id,chain). The history file can be joined to the offers file by (offer). The transactions file can be joined to the offers file by (category, brand, company). A negative value in productquantity and purchaseamount indicates a return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing the Lifetime package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lifetimes\n",
      "  Downloading Lifetimes-0.11.3-py3-none-any.whl (584 kB)\n",
      "Collecting autograd>=1.2.0\n",
      "  Downloading autograd-1.3.tar.gz (38 kB)\n",
      "Requirement already satisfied: pandas>=0.24.0 in c:\\users\\bharg\\datascience\\anaconda3\\lib\\site-packages (from lifetimes) (1.0.3)\n",
      "Collecting dill>=0.2.6\n",
      "  Downloading dill-0.3.2.zip (177 kB)\n",
      "Requirement already satisfied: numpy>=1.10.0 in c:\\users\\bharg\\datascience\\anaconda3\\lib\\site-packages (from lifetimes) (1.18.1)\n",
      "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\bharg\\datascience\\anaconda3\\lib\\site-packages (from lifetimes) (1.4.1)\n",
      "Requirement already satisfied: future>=0.15.2 in c:\\users\\bharg\\datascience\\anaconda3\\lib\\site-packages (from autograd>=1.2.0->lifetimes) (0.18.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\bharg\\datascience\\anaconda3\\lib\\site-packages (from pandas>=0.24.0->lifetimes) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\bharg\\datascience\\anaconda3\\lib\\site-packages (from pandas>=0.24.0->lifetimes) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\bharg\\datascience\\anaconda3\\lib\\site-packages (from python-dateutil>=2.6.1->pandas>=0.24.0->lifetimes) (1.14.0)\n",
      "Building wheels for collected packages: autograd, dill\n",
      "  Building wheel for autograd (setup.py): started\n",
      "  Building wheel for autograd (setup.py): finished with status 'done'\n",
      "  Created wheel for autograd: filename=autograd-1.3-py3-none-any.whl size=47994 sha256=b8049f8a9f28fdaa906c28d6d21adda0970f4d32693b26d31b9e71137476c907\n",
      "  Stored in directory: c:\\users\\bharg\\appdata\\local\\pip\\cache\\wheels\\ef\\32\\31\\0e87227cd0ca1d99ad51fbe4b54c6fa02afccf7e483d045e04\n",
      "  Building wheel for dill (setup.py): started\n",
      "  Building wheel for dill (setup.py): finished with status 'done'\n",
      "  Created wheel for dill: filename=dill-0.3.2-py3-none-any.whl size=78977 sha256=3fe180f21474711fb77947a2994d4275a64932f94d10ec554a92e394752973f2\n",
      "  Stored in directory: c:\\users\\bharg\\appdata\\local\\pip\\cache\\wheels\\72\\6b\\d5\\5548aa1b73b8c3d176ea13f9f92066b02e82141549d90e2100\n",
      "Successfully built autograd dill\n",
      "Installing collected packages: autograd, dill, lifetimes\n",
      "Successfully installed autograd-1.3 dill-0.3.2 lifetimes-0.11.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lifetimes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "#Lifetime package in python is designed to calculate the CLTV \n",
    "import lifetimes\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName(\"Spark Programming\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-UH0JV14P:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Programming</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1e8ab0c16c8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spark)\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the transactions data into the Spark Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the Schema for the datasets\n",
    "# I have kept the ID fields as Nullable=False so that there is no null in these columns as by very basic definition they should be not-null and unique\n",
    "offers_schema =StructType(fields=[StructField('offer',IntegerType(),False),      #offer - An id representing a certain offer\n",
    "                 StructField('category',IntegerType(),True),     #category - The product category (e.g. sparkling water)\n",
    "                StructField('quantity',IntegerType(),True),     #quantity - The number of units one must purchase to get the discount\n",
    "                StructField('company',IntegerType(),False),       #company - An id of the company that sells the item\n",
    "                StructField('offervalue',DoubleType(),True),   #offervalue - The dollar value of the offer\n",
    "                StructField('brand',IntegerType(),False)])         #brand - An id of the brand to which the item belongs\n",
    "\n",
    "transactions_schema = StructType(fields=[StructField('id',StringType(),False),   #id - A unique id representing a customer\n",
    "                      StructField('chain',IntegerType(),True),  #chain - An integer representing a store chain\n",
    "                      StructField('dept',IntegerType(),True),    #dept - An aggregate grouping of the Category (e.g. water)\n",
    "                      StructField('category',IntegerType(),True), #category - The product category (e.g. sparkling water)\n",
    "                      StructField('company',IntegerType(),False), #company - An id of the company that sells the item\n",
    "                      StructField('brand',IntegerType(),False),  #brand - An id of the brand to which the item belongs\n",
    "                      StructField('date',DateType(),True),        #date - The date of purchase\n",
    "                      StructField('productsize',DoubleType(),True), #productsize - The amount of the product purchase (e.g. 16 oz of water)\n",
    "                      StructField('productmeasure',StringType(),True), #productmeasure - The units of the product purchase (e.g. ounces)\n",
    "                      StructField('purchasequantity',IntegerType(),True), #purchasequantity - The number of units purchased\n",
    "                      StructField('purchaseamount',DoubleType(),True)]) #purchaseamount - The dollar amount of the purchase\n",
    "\n",
    "trainHistory_schema =StructType(fields=[StructField('id',StringType(),False),     #id - A unique id representing a customer\n",
    "                      StructField('chain',IntegerType(),True),    #chain - An integer representing a store chain\n",
    "                      StructField('offer',IntegerType(),False),   #offer - An id representing a certain offer\n",
    "                      StructField('market',IntegerType(),False),  #market - An id representing a geographical region\n",
    "                      StructField('repeattrips',IntegerType(),True), #repeattrips - The number of times the customer made a repeat purchase\n",
    "                      StructField('repeater',StringType(),True),  #repeater - A boolean, equal to repeattrips > 0\n",
    "                      StructField('offerdate',DateType(),True)])  #offerdate - The date a customer received the offer\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the data into spark RDDs\n",
    "transactions = spark.read.format('csv').\\\n",
    "                               options(header='true').\\\n",
    "                load(\"Data/X5 Retail Data/acquire-valued-shoppers-challenge/transactions.csv\",header=True,schema=transactions_schema)\n",
    "\n",
    "offers=spark.read.format('csv').\\\n",
    "                               options(header='true').\\\n",
    "                load(\"Data/X5 Retail Data/acquire-valued-shoppers-challenge/offers.csv\",header=True,schema=offers_schema)\n",
    "\n",
    "trainHistory=spark.read.format('csv').\\\n",
    "                               options(header='true').\\\n",
    "                load(\"Data/X5 Retail Data/acquire-valued-shoppers-challenge/trainHistory.csv\",header=True,schema=trainHistory_schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions.select(countDistinct('id')).show()\n",
    "#transactions.show(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging the Datasets for the analysis purpose\n",
    "#Join Transactions to TrainHistory dataset\n",
    "transaction_history=transactions.join(trainHistory,on=['id','chain'],how='inner')\n",
    "history_offer=trainHistory.join(offers,on='offer',how='left')\n",
    "\n",
    " The transactions file can be joined to the offers file by (category, brand, company)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"Transactions Dataset Shape: \\n Number of columns ->\" , transactions.count() , \" and Number of Rows -> \", len(transactions.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the month from the purchase_date column\n",
    "transactions=transactions.withColumn(\"Transaction_Month\", month(transactions['date']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the dataframe to a queriable view. This will allow us to use power of SQL to query the dataframes\n",
    "transactions.createOrReplaceTempView('Transactions') \n",
    "offers.createOrReplaceTempView('Offers')\n",
    "trainHistory.createOrReplaceTempView('History')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1=spark.sql(\"Select TRUNC(date, 'month') as PurchaseMonth,COUNT(DISTINCT chain) as Transactions FROM Transactions GROUP BY TRUNC(date, 'month') ORDER BY PurchaseMonth;\")\n",
    "result1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us explore the transactions dataset first\n",
    "transactions.groupBy(['id','chain','date']).count().show()\n",
    "\n",
    "\n",
    "\n",
    "#Check number of distinct users\n",
    "#spark.sql(\"Select count(distinct(id)) from Transactions\").show()\n",
    "#print(\"Number of customers: %i\" %().collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----+------+-----------+--------+----------+--------+--------+---------+----------+------+\n",
      "|  offer|      id|chain|market|repeattrips|repeater| offerdate|category|quantity|  company|offervalue| brand|\n",
      "+-------+--------+-----+------+-----------+--------+----------+--------+--------+---------+----------+------+\n",
      "|1208251|   86246|  205|    34|          5|       t|2013-04-24|    2202|       1|104460040|       2.0|  3718|\n",
      "|1197502|   86252|  205|    34|         16|       t|2013-03-27|    3203|       1|106414464|      0.75| 13474|\n",
      "|1197502|12682470|   18|    11|          0|       f|2013-03-28|    3203|       1|106414464|      0.75| 13474|\n",
      "|1197502|12996040|   15|     9|          0|       f|2013-03-25|    3203|       1|106414464|      0.75| 13474|\n",
      "|1204821|13089312|   15|     9|          0|       f|2013-04-01|    5619|       1|107717272|       1.5|102504|\n",
      "|1197502|13179265|   14|     8|          0|       f|2013-03-29|    3203|       1|106414464|      0.75| 13474|\n",
      "|1200581|13251776|   15|     9|          0|       f|2013-03-30|    1726|       1|104460040|      1.25|  7668|\n",
      "|1200581|13540129|   14|     8|          0|       f|2013-03-30|    1726|       1|104460040|      1.25|  7668|\n",
      "|1204576|13807224|    4|     1|          0|       f|2013-04-05|    5616|       1|104610040|       1.0| 15889|\n",
      "|1197502|13873775|    4|     1|          0|       f|2013-03-26|    3203|       1|106414464|      0.75| 13474|\n",
      "|1197502|13974451|    4|     1|          0|       f|2013-03-26|    3203|       1|106414464|      0.75| 13474|\n",
      "|1200581|14088807|    3|     2|          1|       t|2013-04-17|    1726|       1|104460040|      1.25|  7668|\n",
      "|1197502|14381137|    4|     1|          0|       f|2013-04-04|    3203|       1|106414464|      0.75| 13474|\n",
      "|1197502|14576147|   18|    11|          0|       f|2013-04-02|    3203|       1|106414464|      0.75| 13474|\n",
      "|1197502|15530842|   95|    39|          0|       f|2013-04-19|    3203|       1|106414464|      0.75| 13474|\n",
      "|1197502|15705695|   15|     9|          0|       f|2013-04-02|    3203|       1|106414464|      0.75| 13474|\n",
      "|1197502|15738658|   17|     4|          0|       f|2013-04-22|    3203|       1|106414464|      0.75| 13474|\n",
      "|1208251|15753725|   17|     4|          0|       f|2013-04-24|    2202|       1|104460040|       2.0|  3718|\n",
      "|1197502|15941277|   14|     8|          1|       t|2013-03-26|    3203|       1|106414464|      0.75| 13474|\n",
      "|1197502|15994113|    4|     1|          0|       f|2013-03-26|    3203|       1|106414464|      0.75| 13474|\n",
      "+-------+--------+-----+------+-----------+--------+----------+--------+--------+---------+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Let us combine the three seperate datasets to create one single dataset\n",
    "merge1=trainHistory.join(offers,on='offer',how='inner')\n",
    "final_df=transactions.join(merge1,on=['id','chain','company','brand','category'],)\n",
    "       # transactions.join(trainHistory.join(offers,on='offer',how='inner'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|repeater|\n",
      "+--------+\n",
      "|    null|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merge1.select('repeater').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us examine the transaction activity\n",
    "%sql\n",
    "select date as Purchase_Date,\n",
    "count(distinct(category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a new dataset with 3 columns: Customer Id, Transaction Date and Sales amount\n",
    "# Sales amount is calculated by multiplying the purchase quantity with the purchase amount\n",
    "sales_df=df2.withColumn(\"Sales\",round(df2.purchasequantity * df2.purchaseamount,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- chain: string (nullable = true)\n",
      " |-- dept: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- company: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- productsize: string (nullable = true)\n",
      " |-- productmeasure: string (nullable = true)\n",
      " |-- purchasequantity: integer (nullable = true)\n",
      " |-- purchaseamount: double (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(349655789, 12)\n"
     ]
    }
   ],
   "source": [
    "# Shape of Sales Dataset\n",
    "print((sales_df.count(), len(sales_df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "311541"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#How many customers are under our analysis?\n",
    "sales_df.select('id').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------------------+\n",
      "|summary|                  id|             Sales|\n",
      "+-------+--------------------+------------------+\n",
      "|  count|           349655789|         349655789|\n",
      "|   mean|1.8395699348116875E9|56.791282085746246|\n",
      "| stddev|1.5394515594486134E9|48228.584970713746|\n",
      "|    min|           100007447|      -6.4676264E7|\n",
      "|    max|            99999754|    4.8358281744E8|\n",
      "+-------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Getting a summary statistics for newly created sales dataset\n",
    "sales_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(datetime.date(2012, 3, 2), datetime.date(2013, 7, 28))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finding the starting date (min_date) and the end date(max_date)\n",
    "min_date, max_date = df2.select(min(\"date\"), max(\"date\")).first()\n",
    "min_date, max_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+-----------+\n",
      "|   id|      date|Sales|RecencyDays|\n",
      "+-----+----------+-----+-----------+\n",
      "|86246|2012-03-02| 7.59|        669|\n",
      "|86246|2012-03-02| 1.59|        669|\n",
      "|86246|2012-03-02| 5.99|        669|\n",
      "|86246|2012-03-02| 1.99|        669|\n",
      "|86246|2012-03-02|20.76|        669|\n",
      "|86246|2012-03-02|  7.8|        669|\n",
      "|86246|2012-03-02| 2.49|        669|\n",
      "|86246|2012-03-02| 1.39|        669|\n",
      "|86246|2012-03-02|  3.0|        669|\n",
      "|86246|2012-03-02| 5.79|        669|\n",
      "|86246|2012-03-02| 0.59|        669|\n",
      "|86246|2012-03-02| 3.29|        669|\n",
      "|86246|2012-03-02| 3.29|        669|\n",
      "|86246|2012-03-02| 1.99|        669|\n",
      "|86246|2012-03-02| 0.89|        669|\n",
      "|86246|2012-03-02| 3.59|        669|\n",
      "|86246|2012-03-02| 3.99|        669|\n",
      "|86246|2012-03-02| 8.87|        669|\n",
      "|86246|2012-03-02| 4.99|        669|\n",
      "|86246|2012-03-02|  1.0|        669|\n",
      "+-----+----------+-----+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculate difference in days between 2013-12-31 and the Invoice Date\n",
    "sales_df=sales_df.withColumn(\"RecencyDays\", expr(\"datediff('2013-12-31', date)\"))\n",
    "sales_df.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
